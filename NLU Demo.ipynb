{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPWKJ0/f/jOKyiHQntrEZ5K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install textstat"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zGbfnaCetcDp","executionInfo":{"status":"ok","timestamp":1713929465565,"user_tz":-60,"elapsed":9651,"user":{"displayName":"Luke","userId":"02926941397281253429"}},"outputId":"0a4c0b56-d3e2-469b-fbb5-11f267459395"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: textstat in /usr/local/lib/python3.10/dist-packages (0.7.3)\n","Requirement already satisfied: pyphen in /usr/local/lib/python3.10/dist-packages (from textstat) (0.15.0)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import tensorflow as tf\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","from nltk.probability import FreqDist\n","from nltk.util import ngrams\n","\n","import zlib\n","import math\n","import textstat\n","import pandas as pd\n","import numpy as np\n","from collections import Counter\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report, cohen_kappa_score\n","from scipy.special import expit\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXp4Qb_9sf3n","executionInfo":{"status":"ok","timestamp":1713959527510,"user_tz":-60,"elapsed":217,"user":{"displayName":"Luke","userId":"02926941397281253429"}},"outputId":"c2918211-4700-43bf-ae0c-d93207e4e1f9"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["def punct(document):\n","    \"\"\"\n","    Creates set of counts of punctuation marks\n","    :param document:\n","    :return:\n","    \"\"\"\n","    punct_counts = [document.count(\"!\") / len(document), document.count(\"?\") / len(document),\n","                    document.count(\";\") / len(document), document.count(\":\") / len(document),\n","                    document.count(\",\") / len(document), document.count(\".\") / len(document),\n","                    document.count(\"-\") / len(document), document.count(\"'\") / len(document)]\n","    return punct_counts\n","\n","def punct_similarity(text1, text2):\n","    punct1 = punct(text1)\n","    punct2 = punct(text2)\n","    return cosine_similarity([punct1], [punct2])[0][0]\n","\n","def compression_based_dissimilarity(doc1, doc2, encoding=\"utf-8\"):\n","    \"\"\"\n","    Get the CDM score (Compression-based Dissimilarity Method) for two documents:\n","      CDM(x, y) =  (C(x) + C(y)) / C(xy)\n","      (see Zhensi-Li p. 19)\n","\n","    Compression algorithm is zlib's gzip.\n","\n","    :param doc1: the first document (as string)\n","    :param doc2: the second document (as string)\n","    :return: the CDM score\n","    \"\"\"\n","    bytes_doc1 = bytes(doc1, encoding)\n","    bytes_doc2 = bytes(doc2, encoding)\n","    return (len(zlib.compress(bytes_doc1)) / len(bytes_doc1)) + (len(zlib.compress(bytes_doc2)) / len(bytes_doc2)) / (len(zlib.compress(bytes_doc1 + bytes_doc2)) / len(bytes_doc1 + bytes_doc2))\n","\n","def char_ngram_similarity(doc1, doc2, n, top=100):\n","    \"\"\"\n","    Gives a positive dissimilarity score of two documents with respect to their top m character n-grams distribution.\n","    If the value is 0 the documents are identical (or at least share an identical top m character n-grams distribution.\n","    :param doc1:\n","    :param doc2:\n","    :param n: the n-gram length\n","    :param top: Only use the N most frequent n-grams from each document.\n","    :return: A positive dissimilarity score. If the value is 0 the documents are identical (or at least their top m\n","             character n-grams distribution.)\n","    \"\"\"\n","\n","    ngrams1 = Counter(ngrams(doc1, n))\n","    ngrams2 = Counter(ngrams(doc2, n))\n","\n","    profile1 = [n[0] for n in ngrams1.most_common(top)]\n","    profile2 = [n[0] for n in ngrams2.most_common(top)]\n","\n","    # normalise the two ngram distributions\n","    total1 = np.sum(list(ngrams1.values()))\n","    for key in ngrams1:\n","        ngrams1[key] /= total1\n","\n","    total2 = np.sum(list(ngrams2.values()))\n","    for key in ngrams2:\n","        ngrams2[key] /= total2\n","\n","    # calculate global dissimilarity score\n","    score = 0\n","    for n in set(profile1 + profile2):\n","        f1 = ngrams1[n]\n","        f2 = ngrams2[n]\n","        score += ((2 * (f1 - f2)) / (f1 + f2)) ** 2\n","    return expit(score)\n","\n","def upper_per_lower(text1, text2):\n","  \"\"\"\n","  Gives a ratio between the number of uppercase characters and lower case characters in the two texts.\n","  :param text1:\n","  :param text1:\n","  :return:\n","  \"\"\"\n","\n","  ratio1 = (len([i for i in text1 if i.isupper()]) + 1e-5) / (len([i for i in text1 if i.islower()]) + 1e-5)\n","  ratio2 = (len([i for i in text2 if i.isupper()]) + 1e-5) / (len([i for i in text2 if i.islower()]) + 1e-5)\n","  return comparison(ratio1, ratio2)\n","def entropy(tokens):\n","    \"\"\"\n","    Get the Shannon entropy of a document using it's token distribution\n","    :param tokens: A document represented as a list of tokens.\n","    :return:\n","    \"\"\"\n","    doc_len = len(tokens)\n","    frq = FreqDist(tokens)\n","    for key in frq.keys():\n","        frq[key] /= doc_len\n","    ent = 0.0\n","    for key in frq.keys():\n","        ent += frq[key] * math.log(frq[key], 2)\n","    ent = -ent\n","    return ent\n","\n","def comparison(num1, num2):\n","  return (num1 - num2 + 1e-5) / (abs(num1) + abs(num2) + 1e-5)\n"],"metadata":{"id":"TM60dNumtLET"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_text_features(text1, text2):\n","  text1 = str(text1)\n","  text2 = str(text2)\n","\n","  num_words1 = len(text1.split())\n","  num_words2 = len(text2.split())\n","  features = {\n","      'length': comparison(len(text1), len(text2)),\n","      'num_words': comparison(num_words1, num_words2),\n","      # 'num_sentences': comparison(len(text1.split(\".\")), len(text2.split(\".\"))),\n","      'avg_word_length': comparison((sum(len(word) for word in text1.split()) / len(text1.split())) , (sum(len(word) for word in text2.split()) / len(text2.split()))),\n","      \"flesch_kincaid_grade\": comparison(textstat.flesch_kincaid_grade(text1), textstat.flesch_kincaid_grade(text2)),\n","      # \"coleman_liau_index\": textstat.coleman_liau_index(text1) - textstat.coleman_liau_index(text2),\n","      # 'automated_readability_index': comparison(textstat.automated_readability_index(text1), textstat.automated_readability_index(text2)),  # Automated Readability Index\n","      # 'ngram_dissimiarlarity' : char_ngram_similarity(text1, text2, 3),\n","      # 'compression_dissimiarlarity' : compression_based_dissimilarity(text1, text2),\n","      'upper_per_lower' : upper_per_lower(text1, text2),\n","      # 'entropy' : comparison(entropy(text1), entropy(text2)),\n","      'punctuation' : punct_similarity(text1, text2),\n","      # Add more textstat\n","      }\n","\n","    # Lexical features using NLTK\n","  tokens1 = word_tokenize(text1)\n","  fdist1 = FreqDist(tokens1)\n","  tokens2 = word_tokenize(text2)\n","  fdist2 = FreqDist(tokens2)\n","  features['unique_words'] = comparison(len(fdist1), len(fdist2))  # Hapax Legomena (unique words)\n","  features['vocab_richness'] =  comparison((len(fdist1) / num_words1), (len(fdist2)/num_words2))  # Type-Token Ratio\n","\n","  # Stylometric features\n","  sentence_lengths1 = [len(s) for s in text1.split(\".\")]\n","  sentence_lengths2 = [len(s) for s in text2.split(\".\")]\n","  features['sentence_length_std'] = comparison((np.std(sentence_lengths1) if sentence_lengths1 else 0), (np.std(sentence_lengths2) if sentence_lengths2 else 0))\n","\n","  # Additional feature - Average character length per word\n","  features['avg_char_per_word'] = comparison((sum(len(word) for word in tokens1) / num_words1), (sum(len(word) for word in tokens2) / num_words2))\n","\n","  return list(features.values())\n"],"metadata":{"id":"whFY1mhb_obO","executionInfo":{"status":"ok","timestamp":1713950277179,"user_tz":-60,"elapsed":240,"user":{"displayName":"Luke","userId":"02926941397281253429"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["# Data preprocessing functions\n","max_features = 15000\n","max_length = 512\n","\n","def clean_text(text):\n","  \"\"\"\n","  Cleans text data by converting to lowercase, removing punctuation,\n","  and removing stop words (optional).\n","  \"\"\"\n","  if type(text) == str:\n","    text = text.lower()\n","    text = ''.join([c for c in text if c.isalnum() or c.isspace()])  # Remove punctuation\n","    return text\n","  return \"\"\n","\n","def preprocess_data(texts, tokenizer):\n","  \"\"\"\n","  Preprocesses text data by cleaning, tokenizing, and padding sequences.\n","  \"\"\"\n","  cleaned_texts = [clean_text(text) for text in texts]  # Clean text\n","  if not tokenizer:\n","    tokenizer = Tokenizer(num_words=max_features)  # Create tokenizer\n","  tokenizer.fit_on_texts(cleaned_texts)  # Fit tokenizer on text data\n","  sequences = tokenizer.texts_to_sequences(cleaned_texts)  # Convert text to sequences\n","  padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')  # Pad sequences\n","\n","  return padded_sequences, tokenizer  # Return padded sequences and tokenizer for encoding unseen text\n","\n"],"metadata":{"id":"0iJKfQ8MZOxq","executionInfo":{"status":"ok","timestamp":1713950323611,"user_tz":-60,"elapsed":251,"user":{"displayName":"Luke","userId":"02926941397281253429"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","execution_count":81,"metadata":{"id":"R4o6jtLIpFMe","executionInfo":{"status":"ok","timestamp":1713950992829,"user_tz":-60,"elapsed":54785,"user":{"displayName":"Luke","userId":"02926941397281253429"}}},"outputs":[],"source":["test_df = pd.read_csv(\"test.csv\", encoding='utf-8', dtype=str)\n","val_df = pd.read_csv(\"dev.csv\", encoding='utf-8')\n","\n","val_texts1, val_texts2, val_labels = val_df[\"text_1\"].tolist(), val_df[\"text_2\"].tolist(), val_df[\"label\"].tolist()  # Load validation data\n","test_texts1, test_texts2 = test_df[\"text_1\"].tolist(), test_df[\"text_2\"].tolist()  # Load test data\n","\n","# Preprocess training, validation, and test data\n","val_data1, tokenizer = preprocess_data(val_texts1, None)\n","val_data2, tokenizer = preprocess_data(val_texts2, tokenizer)  # Reuse tokenizer\n","\n","test_data1, tokenizer = preprocess_data(test_texts1, tokenizer)\n","test_data2, tokenizer = preprocess_data(test_texts2, tokenizer)  # Reuse tokenizer\n","\n","test_df['text_features'] = test_df.apply(lambda row: extract_text_features(row['text_1'], row['text_2']), axis=1)\n","test_text_features = pd.DataFrame(test_df['text_features'].to_list())\n","\n","val_df['text_features'] = val_df.apply(lambda row: extract_text_features(row['text_1'], row['text_2']), axis=1)\n","val_text_features = pd.DataFrame(val_df['text_features'].to_list())\n"]},{"cell_type":"code","source":["model_A = tf.keras.models.load_model('best_model_A.keras')\n","model_B = tf.keras.models.load_model('best_model_B.h5')"],"metadata":{"id":"29M1al9ZsomQ","executionInfo":{"status":"ok","timestamp":1713971073509,"user_tz":-60,"elapsed":3402,"user":{"displayName":"Luke","userId":"02926941397281253429"}}},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation for Model A"],"metadata":{"id":"cNgWpKk5aHDe"}},{"cell_type":"code","source":["predictions = model_A.predict(val_text_features)\n","binary_predictions = (predictions > 0.5).astype(int)\n","\n","y_true = val_df['label']\n","y_pred = binary_predictions\n","\n","# Evaluate model performance (accuracy)\n","accuracy = accuracy_score(y_true, y_pred)\n","print(\"Accuracy:\", accuracy)\n","\n","# # Evaluate model performance (confusion matrix and F1-score)\n","conf_matrix = confusion_matrix(y_true, y_pred)\n","print(\"Confusion Matrix:\\n\", conf_matrix)\n","\n","report = classification_report(y_true, y_pred, output_dict=True)\n","\n","# Access precision and recall for each class\n","precision_class_0 = report['0']['precision']\n","recall_class_0 = report['0']['recall']\n","precision_class_1 = report['1']['precision']\n","recall_class_1 = report['1']['recall']\n","\n","print(\"\\nPrecision (Class 0):\", precision_class_0)\n","print(\"Recall (Class 0):\", recall_class_0)\n","print(\"Precision (Class 1):\", precision_class_1)\n","print(\"Recall (Class 1):\", recall_class_1)\n","\n","f1 = f1_score(y_true, y_pred)\n","print(\"\\nF1-score:\", f1)\n","\n","# MCC\n","tn, fp, fn, tp = conf_matrix.ravel()\n","numerator = tp * tn - fp * fn\n","denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n","mcc = numerator / (denominator + np.spacing(1))\n","print(\"\\nMCC:\", mcc)\n","\n","# Cohen's Kappa\n","kappa = cohen_kappa_score(y_true, y_pred)\n","print(\"\\nCohen's Kappa:\", kappa)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CB40BrHb66V0","executionInfo":{"status":"ok","timestamp":1713971074611,"user_tz":-60,"elapsed":1105,"user":{"displayName":"Luke","userId":"02926941397281253429"}},"outputId":"52a50669-1641-4775-cf9c-4262baf6e3c2"},"execution_count":104,"outputs":[{"output_type":"stream","name":"stdout","text":["188/188 [==============================] - 0s 2ms/step\n","Accuracy: 0.5818333333333333\n","Confusion Matrix:\n"," [[1931 1058]\n"," [1451 1560]]\n","\n","Precision (Class 0): 0.5709639266706091\n","Recall (Class 0): 0.6460354633656742\n","Precision (Class 1): 0.5958747135217723\n","Recall (Class 1): 0.5181002989040187\n","\n","F1-score: 0.5542725173210161\n","\n","MCC: 0.16548168292598286\n","\n","Cohen's Kappa: 0.16405695918416763\n"]}]},{"cell_type":"markdown","source":["# Evaluation for Model B"],"metadata":{"id":"8ovuMUsKaKvP"}},{"cell_type":"code","source":["predictions = model_B.predict([np.array(val_data1), np.array(val_data2)])\n","binary_predictions = (predictions > 0.5).astype(int)\n","\n","y_true = val_df['label']\n","y_pred = binary_predictions\n","\n","# Evaluate model performance (accuracy)\n","accuracy = accuracy_score(y_true, y_pred)\n","print(\"Accuracy:\", accuracy)\n","\n","# Evaluate model performance (confusion matrix and F1-score)\n","conf_matrix = confusion_matrix(y_true, y_pred)\n","print(\"Confusion Matrix:\\n\", conf_matrix)\n","\n","report = classification_report(y_true, y_pred, output_dict=True)\n","\n","# Access precision and recall for each class\n","precision_class_0 = report['0']['precision']\n","recall_class_0 = report['0']['recall']\n","precision_class_1 = report['1']['precision']\n","recall_class_1 = report['1']['recall']\n","\n","print(\"\\nPrecision (Class 0):\", precision_class_0)\n","print(\"Recall (Class 0):\", recall_class_0)\n","print(\"Precision (Class 1):\", precision_class_1)\n","print(\"Recall (Class 1):\", recall_class_1)\n","\n","f1 = f1_score(y_true, y_pred)\n","print(\"\\nF1-score:\", f1)\n","\n","# MCC\n","tn, fp, fn, tp = conf_matrix.ravel()\n","numerator = tp * tn - fp * fn\n","denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n","mcc = numerator / (denominator + np.spacing(1))\n","print(\"\\nMCC:\", mcc)\n","\n","# Cohen's Kappa\n","kappa = cohen_kappa_score(y_true, y_pred)\n","print(\"\\nCohen's Kappa:\", kappa)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3iNKT3naNWG","executionInfo":{"status":"ok","timestamp":1713971136777,"user_tz":-60,"elapsed":58705,"user":{"displayName":"Luke","userId":"02926941397281253429"}},"outputId":"0931e450-0430-47d2-f7ba-607dd1603343"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["188/188 [==============================] - 58s 300ms/step\n","Accuracy: 0.5195\n","Confusion Matrix:\n"," [[1358 1631]\n"," [1252 1759]]\n","\n","Precision (Class 0): 0.5203065134099617\n","Recall (Class 0): 0.45433255269320844\n","Precision (Class 1): 0.5188790560471976\n","Recall (Class 1): 0.584191298571903\n","\n","F1-score: 0.5496016247461335\n","\n","MCC: 0.038853301655152814\n","\n","Cohen's Kappa: 0.038541704879325844\n"]}]},{"cell_type":"markdown","source":["# Predictions for Model A"],"metadata":{"id":"UsUF7TrPaTyz"}},{"cell_type":"code","source":["predictions = model_A.predict(test_text_features)\n","binary_predictions = (predictions > 0.5).astype(int)\n","\n","output_df = pd.DataFrame(binary_predictions, columns=[\"prediction\"])\n","print(output_df)\n","output_df.to_csv(\"Group_51_A.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4M2cwA5t4_Q","executionInfo":{"status":"ok","timestamp":1713971138394,"user_tz":-60,"elapsed":1619,"user":{"displayName":"Luke","userId":"02926941397281253429"}},"outputId":"fbb00812-4bf5-4835-ed13-99fe84def55f"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stdout","text":["188/188 [==============================] - 1s 5ms/step\n","      prediction\n","0              1\n","1              0\n","2              0\n","3              1\n","4              0\n","...          ...\n","5995           0\n","5996           1\n","5997           0\n","5998           0\n","5999           1\n","\n","[6000 rows x 1 columns]\n"]}]},{"cell_type":"markdown","source":["# Predictions for Model B"],"metadata":{"id":"muh6pm12aW6-"}},{"cell_type":"code","source":["predictions = model_B.predict([np.array(test_data1), np.array(test_data2)])\n","binary_predictions = (predictions > 0.5).astype(int)\n","\n","output_df = pd.DataFrame(binary_predictions, columns=[\"prediction\"])\n","print(output_df)\n","output_df.to_csv(\"Group_51_B.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJtE2SV6xnqo","executionInfo":{"status":"ok","timestamp":1713971266816,"user_tz":-60,"elapsed":82644,"user":{"displayName":"Luke","userId":"02926941397281253429"}},"outputId":"f7bc176a-ea0a-48c7-ee0f-1fe4100d333d"},"execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["188/188 [==============================] - 68s 360ms/step\n","      prediction\n","0              0\n","1              0\n","2              1\n","3              1\n","4              0\n","...          ...\n","5995           1\n","5996           1\n","5997           1\n","5998           1\n","5999           1\n","\n","[6000 rows x 1 columns]\n"]}]}]}