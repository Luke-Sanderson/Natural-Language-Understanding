{"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"yx7kgdNo84OE"}},{"cell_type":"markdown","metadata":{"id":"-4l9ejER4z37"},"source":["# Imports and Stuff"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zoYRkdA4yb9"},"outputs":[],"source":["import tensorflow as tf\n","\n","from tensorflow.keras.layers import Input, Dot, LSTM, Bidirectional, Dense, Embedding, Lambda\n","from tensorflow.keras import backend, Model\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"GLQ6q7eb6OWG"},"source":["# Hyper parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B33QGS0L6QmB"},"outputs":[],"source":["max_length = 512\n","BATCH_SIZE = 64\n","EPOCHS=20\n","max_features = 15000\n","embedding_dim = 512\n","LEARNING_RATE = 0.01\n","DROPOUT = 0.1"]},{"cell_type":"markdown","metadata":{"id":"uCSA9IpV5uk7"},"source":["# Data loading and pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DiRs3yGJs254"},"outputs":[],"source":["# Data preprocessing functions\n","def clean_text(text):\n","  \"\"\"\n","  Cleans text data by converting to lowercase, removing punctuation,\n","  and removing stop words (optional).\n","  \"\"\"\n","  if type(text) == str:\n","    text = text.lower()\n","    text = ''.join([c for c in text if c.isalnum() or c.isspace()])  # Remove punctuation\n","    return text\n","  return \"\"\n","\n","def preprocess_data(texts, tokenizer):\n","  \"\"\"\n","  Preprocesses text data by cleaning, tokenizing, and padding sequences.\n","  \"\"\"\n","  cleaned_texts = [clean_text(text) for text in texts]  # Clean text\n","  if not tokenizer:\n","    tokenizer = Tokenizer(num_words=max_features)  # Create tokenizer\n","  tokenizer.fit_on_texts(cleaned_texts)  # Fit tokenizer on text data\n","  sequences = tokenizer.texts_to_sequences(cleaned_texts)  # Convert text to sequences\n","  padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')  # Pad sequences\n","\n","  return padded_sequences, tokenizer  # Return padded sequences and tokenizer for encoding unseen text\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DK2tZ5gEs6M6"},"outputs":[],"source":["train_df = pd.read_csv(\"train.csv\").dropna()\n","val_df = pd.read_csv(\"dev.csv\").dropna()\n","test_df = pd.read_csv(\"AV_trial.csv\").dropna()\n","\n","train_texts1, train_texts2, train_labels = train_df[\"text_1\"].tolist(), train_df[\"text_2\"].tolist(), train_df[\"label\"].tolist()  # Load training data (pair of texts and labels)\n","val_texts1, val_texts2, val_labels = val_df[\"text_1\"].tolist(), val_df[\"text_2\"].tolist(), val_df[\"label\"].tolist()  # Load validation data\n","test_texts1, test_texts2, test_labels = test_df[\"text_1\"].tolist(), test_df[\"text_2\"].tolist(), test_df[\"label\"].tolist()  # Load test data\n","\n","# Preprocess training, validation, and test data\n","train_data1, tokenizer = preprocess_data(train_texts1, None)\n","train_data2, tokenizer = preprocess_data(train_texts2, tokenizer)  # Reuse tokenizer\n","\n","val_data1, val_tokenizer = preprocess_data(val_texts1, tokenizer)\n","val_data2, tokenizer = preprocess_data(val_texts2, val_tokenizer)  # Reuse tokenizer\n","\n","test_data1, test_tokenizer = preprocess_data(test_texts1, tokenizer)\n","test_data2, tokenizer = preprocess_data(test_texts2, test_tokenizer)  # Reuse tokenizer\n","\n","# Get vocabulary size (considering all data for better coverage)\n","all_texts = train_texts1 + train_texts2 + val_texts1 + val_texts2 + test_texts1 + test_texts2\n","cleaned_texts = [clean_text(text) for text in all_texts]  # Clean text\n","\n","tokenizer.fit_on_texts(cleaned_texts)  # Fit tokenizer on text data\n","vocab_size = len(tokenizer.word_index) + 1  # Preprocess and get vocabulary size\n","\n"]},{"cell_type":"markdown","source":["# Model Definition"],"metadata":{"id":"Nlyzc_2Bv0i3"}},{"cell_type":"code","source":["from gensim.models import KeyedVectors\n","from keras import regularizers\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","np.random.seed(42)\n","embedding_dim = 300\n","\n","# Defining input shapes\n","input_shape = (max_length,)\n","\n","# Loading pre-trained Word2Vec model\n","word2vec_model = KeyedVectors.load_word2vec_format('/content/GoogleNews-vectors-negative300.bin.gz', binary=True)\n","word_index= tokenizer.word_index\n","\n","# Creating an embedding matrix\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","for word, i in word_index.items():\n","    if word in word2vec_model:\n","        embedding_matrix[i] = word2vec_model[word]\n","\n","embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)\n","\n","# Defining a shared GRU layer\n","gru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32, dropout=0.3))\n","\n","# Define input layers for two texts\n","input_text1 = tf.keras.layers.Input(shape=input_shape, name='input_text1')\n","input_text2 = tf.keras.layers.Input(shape=input_shape, name='input_text2')\n","\n","# Applying shared embedding and GRU layers to both inputs\n","embedded_text1 = embedding_layer(input_text1)\n","embedded_text2 = embedding_layer(input_text2)\n","gru_output1 = gru_layer(embedded_text1)\n","gru_output2 = gru_layer(embedded_text2)\n","\n","# Concatenating GRU outputs\n","concatenated_output = tf.keras.layers.Concatenate()([gru_output1, gru_output2])\n","\n","# Applying Batch Normalization\n","bn_layer = tf.keras.layers.BatchNormalization()\n","bn_output = bn_layer(concatenated_output)\n","\n","# Adding Dense layers for classification with L2 regularization\n","x = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(bn_output)\n","x = tf.keras.layers.Dropout(0.5)(x)  # Adding dropout layer\n","output = tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n","\n","# Creating the Siamese GRU model\n","siamese_model = tf.keras.Model(inputs=[input_text1, input_text2], outputs=output)\n","\n","model_checkpoint = ModelCheckpoint(filepath='best_model_B.h5', monitor='val_accuracy', save_best_only=True)\n","\n","\n","# Compiling the model\n","siamese_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n","                      loss='binary_crossentropy',\n","                      metrics=['accuracy'])\n","\n","# Display model summary\n","siamese_model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XVoPSJ7qVdhr","executionInfo":{"status":"ok","timestamp":1713962910989,"user_tz":-60,"elapsed":55238,"user":{"displayName":"sarah saad","userId":"13788132756993113918"}},"outputId":"9ea05ea7-bc58-41b4-e2d1-c1005fcbd692"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_text1 (InputLayer)    [(None, 512)]                0         []                            \n","                                                                                                  \n"," input_text2 (InputLayer)    [(None, 512)]                0         []                            \n","                                                                                                  \n"," embedding_5 (Embedding)     (None, 512, 300)             4226280   ['input_text1[0][0]',         \n","                                                          0          'input_text2[0][0]']         \n","                                                                                                  \n"," bidirectional_9 (Bidirecti  (None, 64)                   64128     ['embedding_5[0][0]',         \n"," onal)                                                               'embedding_5[1][0]']         \n","                                                                                                  \n"," concatenate_7 (Concatenate  (None, 128)                  0         ['bidirectional_9[0][0]',     \n"," )                                                                   'bidirectional_9[1][0]']     \n","                                                                                                  \n"," batch_normalization_9 (Bat  (None, 128)                  512       ['concatenate_7[0][0]']       \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," dense_6 (Dense)             (None, 64)                   8256      ['batch_normalization_9[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," dropout_2 (Dropout)         (None, 64)                   0         ['dense_6[0][0]']             \n","                                                                                                  \n"," dense_7 (Dense)             (None, 1)                    65        ['dropout_2[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 42335761 (161.50 MB)\n","Trainable params: 72705 (284.00 KB)\n","Non-trainable params: 42263056 (161.22 MB)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["# Model Training"],"metadata":{"id":"-hJUfCTJv48j"}},{"cell_type":"code","source":["siamese_model.fit([np.array(train_data1), np.array(train_data2)],\n","          np.array(train_labels),\n","          epochs=100,\n","          validation_data=([np.array(val_data1), np.array(val_data2)],\n","                           np.array(val_labels)),\n","                  callbacks=[model_checkpoint])\n"],"metadata":{"id":"xQQR6M77yHDH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f78ebf73-65a2-4983-c50d-d393b50766be","executionInfo":{"status":"ok","timestamp":1713968944149,"user_tz":-60,"elapsed":108139,"user":{"displayName":"sarah saad","userId":"13788132756993113918"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","936/936 [==============================] - ETA: 0s - loss: 1.3458 - accuracy: 0.4999"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["936/936 [==============================] - 73s 70ms/step - loss: 1.3458 - accuracy: 0.4999 - val_loss: 1.0782 - val_accuracy: 0.5058\n","Epoch 2/100\n","936/936 [==============================] - 59s 63ms/step - loss: 1.0116 - accuracy: 0.5068 - val_loss: 0.9165 - val_accuracy: 0.5025\n","Epoch 3/100\n","936/936 [==============================] - 59s 64ms/step - loss: 0.8953 - accuracy: 0.5062 - val_loss: 0.8474 - val_accuracy: 0.5037\n","Epoch 4/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.8303 - accuracy: 0.5153 - val_loss: 0.8040 - val_accuracy: 0.5050\n","Epoch 5/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.7885 - accuracy: 0.5196 - val_loss: 0.7725 - val_accuracy: 0.5043\n","Epoch 6/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.7586 - accuracy: 0.5252 - val_loss: 0.7495 - val_accuracy: 0.5015\n","Epoch 7/100\n","936/936 [==============================] - 58s 61ms/step - loss: 0.7370 - accuracy: 0.5304 - val_loss: 0.7331 - val_accuracy: 0.5007\n","Epoch 8/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.7223 - accuracy: 0.5320 - val_loss: 0.7214 - val_accuracy: 0.5012\n","Epoch 9/100\n","936/936 [==============================] - 64s 68ms/step - loss: 0.7117 - accuracy: 0.5370 - val_loss: 0.7133 - val_accuracy: 0.5077\n","Epoch 10/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.7046 - accuracy: 0.5384 - val_loss: 0.7078 - val_accuracy: 0.5072\n","Epoch 11/100\n","936/936 [==============================] - 58s 62ms/step - loss: 0.6994 - accuracy: 0.5409 - val_loss: 0.7042 - val_accuracy: 0.5065\n","Epoch 12/100\n","936/936 [==============================] - 64s 69ms/step - loss: 0.6962 - accuracy: 0.5414 - val_loss: 0.7016 - val_accuracy: 0.5152\n","Epoch 13/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6936 - accuracy: 0.5438 - val_loss: 0.7006 - val_accuracy: 0.5124\n","Epoch 14/100\n","936/936 [==============================] - 58s 62ms/step - loss: 0.6919 - accuracy: 0.5467 - val_loss: 0.6998 - val_accuracy: 0.5129\n","Epoch 15/100\n","936/936 [==============================] - 65s 69ms/step - loss: 0.6896 - accuracy: 0.5522 - val_loss: 0.7002 - val_accuracy: 0.5169\n","Epoch 16/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6889 - accuracy: 0.5512 - val_loss: 0.7002 - val_accuracy: 0.5167\n","Epoch 17/100\n","936/936 [==============================] - 65s 70ms/step - loss: 0.6875 - accuracy: 0.5565 - val_loss: 0.7000 - val_accuracy: 0.5217\n","Epoch 18/100\n","936/936 [==============================] - 64s 69ms/step - loss: 0.6868 - accuracy: 0.5570 - val_loss: 0.7004 - val_accuracy: 0.5232\n","Epoch 19/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6855 - accuracy: 0.5629 - val_loss: 0.7007 - val_accuracy: 0.5216\n","Epoch 20/100\n","936/936 [==============================] - 63s 68ms/step - loss: 0.6847 - accuracy: 0.5617 - val_loss: 0.7008 - val_accuracy: 0.5284\n","Epoch 21/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6827 - accuracy: 0.5652 - val_loss: 0.7014 - val_accuracy: 0.5281\n","Epoch 22/100\n","936/936 [==============================] - 58s 62ms/step - loss: 0.6825 - accuracy: 0.5665 - val_loss: 0.7015 - val_accuracy: 0.5266\n","Epoch 23/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6808 - accuracy: 0.5729 - val_loss: 0.7018 - val_accuracy: 0.5232\n","Epoch 24/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6804 - accuracy: 0.5739 - val_loss: 0.7015 - val_accuracy: 0.5277\n","Epoch 25/100\n","936/936 [==============================] - 63s 68ms/step - loss: 0.6802 - accuracy: 0.5730 - val_loss: 0.7018 - val_accuracy: 0.5318\n","Epoch 26/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6787 - accuracy: 0.5763 - val_loss: 0.7017 - val_accuracy: 0.5261\n","Epoch 27/100\n","936/936 [==============================] - 58s 62ms/step - loss: 0.6788 - accuracy: 0.5775 - val_loss: 0.7019 - val_accuracy: 0.5292\n","Epoch 28/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.6774 - accuracy: 0.5782 - val_loss: 0.7023 - val_accuracy: 0.5296\n","Epoch 29/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6772 - accuracy: 0.5773 - val_loss: 0.7024 - val_accuracy: 0.5266\n","Epoch 30/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6765 - accuracy: 0.5823 - val_loss: 0.7023 - val_accuracy: 0.5296\n","Epoch 31/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6760 - accuracy: 0.5855 - val_loss: 0.7022 - val_accuracy: 0.5286\n","Epoch 32/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.6727 - accuracy: 0.5898 - val_loss: 0.7036 - val_accuracy: 0.5254\n","Epoch 33/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6737 - accuracy: 0.5893 - val_loss: 0.7034 - val_accuracy: 0.5277\n","Epoch 34/100\n","936/936 [==============================] - 58s 62ms/step - loss: 0.6725 - accuracy: 0.5919 - val_loss: 0.7044 - val_accuracy: 0.5239\n","Epoch 35/100\n","936/936 [==============================] - 60s 65ms/step - loss: 0.6711 - accuracy: 0.5928 - val_loss: 0.7043 - val_accuracy: 0.5227\n","Epoch 36/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6706 - accuracy: 0.5947 - val_loss: 0.7052 - val_accuracy: 0.5307\n","Epoch 37/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6686 - accuracy: 0.6002 - val_loss: 0.7067 - val_accuracy: 0.5306\n","Epoch 38/100\n","936/936 [==============================] - 58s 62ms/step - loss: 0.6686 - accuracy: 0.5981 - val_loss: 0.7057 - val_accuracy: 0.5206\n","Epoch 39/100\n","936/936 [==============================] - 63s 67ms/step - loss: 0.6674 - accuracy: 0.5985 - val_loss: 0.7057 - val_accuracy: 0.5326\n","Epoch 40/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.6669 - accuracy: 0.5992 - val_loss: 0.7075 - val_accuracy: 0.5201\n","Epoch 41/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6656 - accuracy: 0.6019 - val_loss: 0.7084 - val_accuracy: 0.5301\n","Epoch 42/100\n","936/936 [==============================] - 59s 64ms/step - loss: 0.6643 - accuracy: 0.5994 - val_loss: 0.7083 - val_accuracy: 0.5261\n","Epoch 43/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6648 - accuracy: 0.6035 - val_loss: 0.7104 - val_accuracy: 0.5284\n","Epoch 44/100\n","936/936 [==============================] - 62s 66ms/step - loss: 0.6610 - accuracy: 0.6073 - val_loss: 0.7142 - val_accuracy: 0.5338\n","Epoch 45/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6616 - accuracy: 0.6043 - val_loss: 0.7106 - val_accuracy: 0.5207\n","Epoch 46/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6596 - accuracy: 0.6085 - val_loss: 0.7096 - val_accuracy: 0.5307\n","Epoch 47/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6579 - accuracy: 0.6121 - val_loss: 0.7110 - val_accuracy: 0.5336\n","Epoch 48/100\n","936/936 [==============================] - 62s 67ms/step - loss: 0.6572 - accuracy: 0.6137 - val_loss: 0.7153 - val_accuracy: 0.5343\n","Epoch 49/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6561 - accuracy: 0.6153 - val_loss: 0.7131 - val_accuracy: 0.5242\n","Epoch 50/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6555 - accuracy: 0.6156 - val_loss: 0.7130 - val_accuracy: 0.5224\n","Epoch 51/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.6538 - accuracy: 0.6144 - val_loss: 0.7125 - val_accuracy: 0.5324\n","Epoch 52/100\n","936/936 [==============================] - 62s 66ms/step - loss: 0.6533 - accuracy: 0.6185 - val_loss: 0.7166 - val_accuracy: 0.5356\n","Epoch 53/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6506 - accuracy: 0.6246 - val_loss: 0.7134 - val_accuracy: 0.5209\n","Epoch 54/100\n","936/936 [==============================] - 58s 62ms/step - loss: 0.6505 - accuracy: 0.6215 - val_loss: 0.7148 - val_accuracy: 0.5281\n","Epoch 55/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6481 - accuracy: 0.6289 - val_loss: 0.7136 - val_accuracy: 0.5307\n","Epoch 56/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6483 - accuracy: 0.6262 - val_loss: 0.7173 - val_accuracy: 0.5348\n","Epoch 57/100\n","936/936 [==============================] - 58s 62ms/step - loss: 0.6463 - accuracy: 0.6277 - val_loss: 0.7181 - val_accuracy: 0.5261\n","Epoch 58/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6454 - accuracy: 0.6261 - val_loss: 0.7160 - val_accuracy: 0.5274\n","Epoch 59/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.6446 - accuracy: 0.6298 - val_loss: 0.7211 - val_accuracy: 0.5236\n","Epoch 60/100\n","936/936 [==============================] - 62s 66ms/step - loss: 0.6409 - accuracy: 0.6343 - val_loss: 0.7236 - val_accuracy: 0.5160\n","Epoch 61/100\n","936/936 [==============================] - 65s 70ms/step - loss: 0.6406 - accuracy: 0.6307 - val_loss: 0.7388 - val_accuracy: 0.5242\n","Epoch 62/100\n","936/936 [==============================] - 62s 66ms/step - loss: 0.6395 - accuracy: 0.6315 - val_loss: 0.7230 - val_accuracy: 0.5264\n","Epoch 63/100\n","936/936 [==============================] - 62s 66ms/step - loss: 0.6386 - accuracy: 0.6357 - val_loss: 0.7232 - val_accuracy: 0.5179\n","Epoch 64/100\n","936/936 [==============================] - 66s 71ms/step - loss: 0.6388 - accuracy: 0.6351 - val_loss: 0.7286 - val_accuracy: 0.5247\n","Epoch 65/100\n","936/936 [==============================] - 61s 65ms/step - loss: 0.6362 - accuracy: 0.6395 - val_loss: 0.7327 - val_accuracy: 0.5227\n","Epoch 66/100\n","936/936 [==============================] - 66s 71ms/step - loss: 0.6371 - accuracy: 0.6398 - val_loss: 0.7260 - val_accuracy: 0.5249\n","Epoch 67/100\n","936/936 [==============================] - 61s 65ms/step - loss: 0.6361 - accuracy: 0.6367 - val_loss: 0.7291 - val_accuracy: 0.5286\n","Epoch 68/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6348 - accuracy: 0.6382 - val_loss: 0.7303 - val_accuracy: 0.5122\n","Epoch 69/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6327 - accuracy: 0.6426 - val_loss: 0.7331 - val_accuracy: 0.5209\n","Epoch 70/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6328 - accuracy: 0.6440 - val_loss: 0.7388 - val_accuracy: 0.5154\n","Epoch 71/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.6335 - accuracy: 0.6394 - val_loss: 0.7906 - val_accuracy: 0.5058\n","Epoch 72/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6305 - accuracy: 0.6443 - val_loss: 0.7347 - val_accuracy: 0.5209\n","Epoch 73/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6292 - accuracy: 0.6452 - val_loss: 0.7365 - val_accuracy: 0.5182\n","Epoch 74/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6308 - accuracy: 0.6446 - val_loss: 0.7341 - val_accuracy: 0.5212\n","Epoch 75/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.6291 - accuracy: 0.6476 - val_loss: 0.7402 - val_accuracy: 0.5099\n","Epoch 76/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6279 - accuracy: 0.6495 - val_loss: 0.7449 - val_accuracy: 0.5154\n","Epoch 77/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6284 - accuracy: 0.6483 - val_loss: 0.7409 - val_accuracy: 0.5239\n","Epoch 78/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6260 - accuracy: 0.6508 - val_loss: 0.7415 - val_accuracy: 0.5130\n","Epoch 79/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6232 - accuracy: 0.6521 - val_loss: 0.7368 - val_accuracy: 0.5236\n","Epoch 80/100\n","936/936 [==============================] - 64s 69ms/step - loss: 0.6246 - accuracy: 0.6525 - val_loss: 0.7450 - val_accuracy: 0.5110\n","Epoch 81/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.6252 - accuracy: 0.6536 - val_loss: 0.7446 - val_accuracy: 0.5145\n","Epoch 82/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6218 - accuracy: 0.6554 - val_loss: 0.7643 - val_accuracy: 0.5092\n","Epoch 83/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6231 - accuracy: 0.6519 - val_loss: 0.7415 - val_accuracy: 0.5164\n","Epoch 84/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.6211 - accuracy: 0.6547 - val_loss: 0.7542 - val_accuracy: 0.5107\n","Epoch 85/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.6221 - accuracy: 0.6538 - val_loss: 0.7443 - val_accuracy: 0.5102\n","Epoch 86/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6205 - accuracy: 0.6584 - val_loss: 0.7413 - val_accuracy: 0.5115\n","Epoch 87/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.6193 - accuracy: 0.6583 - val_loss: 0.7586 - val_accuracy: 0.5137\n","Epoch 88/100\n","936/936 [==============================] - 58s 62ms/step - loss: 0.6186 - accuracy: 0.6570 - val_loss: 0.7486 - val_accuracy: 0.5055\n","Epoch 89/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6195 - accuracy: 0.6584 - val_loss: 0.7527 - val_accuracy: 0.5122\n","Epoch 90/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6186 - accuracy: 0.6578 - val_loss: 0.7479 - val_accuracy: 0.5038\n","Epoch 91/100\n","936/936 [==============================] - 58s 62ms/step - loss: 0.6183 - accuracy: 0.6572 - val_loss: 0.7445 - val_accuracy: 0.5125\n","Epoch 92/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6158 - accuracy: 0.6623 - val_loss: 0.7482 - val_accuracy: 0.5160\n","Epoch 93/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6153 - accuracy: 0.6637 - val_loss: 0.7600 - val_accuracy: 0.5102\n","Epoch 94/100\n","936/936 [==============================] - 58s 62ms/step - loss: 0.6154 - accuracy: 0.6644 - val_loss: 0.7590 - val_accuracy: 0.5135\n","Epoch 95/100\n","936/936 [==============================] - 59s 63ms/step - loss: 0.6153 - accuracy: 0.6608 - val_loss: 0.7606 - val_accuracy: 0.5082\n","Epoch 96/100\n","936/936 [==============================] - 58s 62ms/step - loss: 0.6142 - accuracy: 0.6648 - val_loss: 0.7629 - val_accuracy: 0.5107\n","Epoch 97/100\n","936/936 [==============================] - 57s 61ms/step - loss: 0.6136 - accuracy: 0.6648 - val_loss: 0.7550 - val_accuracy: 0.5114\n","Epoch 98/100\n","936/936 [==============================] - 58s 62ms/step - loss: 0.6139 - accuracy: 0.6636 - val_loss: 0.7555 - val_accuracy: 0.5150\n","Epoch 99/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.6115 - accuracy: 0.6646 - val_loss: 0.7541 - val_accuracy: 0.5090\n","Epoch 100/100\n","936/936 [==============================] - 60s 64ms/step - loss: 0.6126 - accuracy: 0.6677 - val_loss: 0.7593 - val_accuracy: 0.5159\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7a3ebdcb9990>"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["# Create table data\n","table_data = [\n","    [\"Dropout Only\", \"0.5038\"],\n","    [\"Adding Regularization and Batch Normalization\", \"0.5189\"],\n","    [\"Adding Word2Vec\", \"0.5440\"]\n","]\n","\n","# Create HTML table with custom styling\n","html_table = \"<table style='border-collapse: collapse;'>\"\n","html_table += \"<tr><th style='border: 2px solid #FF0A9D; padding: 10px; color: #1F4DBF;'>Method</th>\"\n","html_table += \"<th style='border: 2px solid #FF0A9D; padding: 10px; color: #1F4DBF;'>Val Accuracy</th></tr>\"\n","\n","for row in table_data:\n","    html_table += \"<tr>\"\n","    for cell in row:\n","        html_table += f\"<td style='border: 2px solid #FF0A9D; padding: 10px; color: #1F4DBF;'>{cell}</td>\"\n","    html_table += \"</tr>\"\n","\n","html_table += \"</table>\"\n","\n","# Display HTML table\n","from IPython.display import HTML\n","HTML(html_table)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"-7fhn7n3FJVD","executionInfo":{"status":"ok","timestamp":1713945063482,"user_tz":-60,"elapsed":11,"user":{"displayName":"sarah saad","userId":"13788132756993113918"}},"outputId":"a16f3d17-4e2c-4fad-e510-d3e0423efbd3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table style='border-collapse: collapse;'><tr><th style='border: 2px solid #FF0A9D; padding: 10px; color: #1F4DBF;'>Method</th><th style='border: 2px solid #FF0A9D; padding: 10px; color: #1F4DBF;'>Val Accuracy</th></tr><tr><td style='border: 2px solid #FF0A9D; padding: 10px; color: #1F4DBF;'>Dropout Only</td><td style='border: 2px solid #FF0A9D; padding: 10px; color: #1F4DBF;'>0.5038</td></tr><tr><td style='border: 2px solid #FF0A9D; padding: 10px; color: #1F4DBF;'>Adding Regularization and Batch Normalization</td><td style='border: 2px solid #FF0A9D; padding: 10px; color: #1F4DBF;'>0.5189</td></tr><tr><td style='border: 2px solid #FF0A9D; padding: 10px; color: #1F4DBF;'>Adding Word2Vec</td><td style='border: 2px solid #FF0A9D; padding: 10px; color: #1F4DBF;'>0.5440</td></tr></table>"]},"metadata":{},"execution_count":48}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}